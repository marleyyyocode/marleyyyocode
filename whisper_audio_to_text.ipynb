{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ™ï¸ Audio to Text with OpenAI Whisper\n",
        "\n",
        "This notebook uses [OpenAI Whisper](https://github.com/openai/whisper) to transcribe audio files to text.\n",
        "\n",
        "**Supported languages:** English ðŸ‡ºðŸ‡¸ and Spanish ðŸ‡ªðŸ‡¸\n",
        "\n",
        "## Setup\n",
        "\n",
        "1. Go to **Runtime â†’ Change runtime type** and select **GPU** (T4 recommended) for faster transcription.\n",
        "2. Run the cells below in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q openai-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import whisper\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load the Whisper Model\n",
        "\n",
        "Available model sizes (larger = more accurate but slower):\n",
        "\n",
        "| Model  | Parameters | Relative Speed | Notes |\n",
        "|--------|-----------|----------------|-------|\n",
        "| tiny   | 39M       | ~32x           | Fastest, least accurate |\n",
        "| base   | 74M       | ~16x           | Good balance for short clips |\n",
        "| small  | 244M      | ~6x            | Good accuracy |\n",
        "| medium | 769M      | ~2x            | Great for Spanish |\n",
        "| large  | 1550M     | 1x             | Best accuracy, slowest |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Select Model Size { run: \"auto\" }\n",
        "model_size = \"medium\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
        "\n",
        "print(f\"Loading Whisper '{model_size}' model...\")\n",
        "model = whisper.load_model(model_size, device=device)\n",
        "print(f\"Model '{model_size}' loaded successfully on {device}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload Your Audio File\n",
        "\n",
        "Supported formats: `.mp3`, `.wav`, `.m4a`, `.flac`, `.ogg`, `.webm`, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded: {audio_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transcribe Audio\n",
        "\n",
        "Choose the language of your audio:\n",
        "- `English` â€” transcribe English audio\n",
        "- `Spanish` â€” transcribe Spanish audio\n",
        "- `Auto-detect` â€” let Whisper detect the language automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Transcription Settings { run: \"auto\" }\n",
        "language = \"Auto-detect\" #@param [\"Auto-detect\", \"English\", \"Spanish\"]\n",
        "\n",
        "language_map = {\n",
        "    \"English\": \"en\",\n",
        "    \"Spanish\": \"es\",\n",
        "    \"Auto-detect\": None\n",
        "}\n",
        "lang_code = language_map[language]\n",
        "\n",
        "print(f\"Transcribing '{audio_file}' (language: {language})...\")\n",
        "result = model.transcribe(audio_file, language=lang_code)\n",
        "\n",
        "detected_lang = result.get(\"language\", \"unknown\")\n",
        "print(f\"Detected language: {detected_lang}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRANSCRIPTION\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. View Timestamped Segments\n",
        "\n",
        "Each segment includes start/end times and the corresponding text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'Start':>8}  {'End':>8}  Text\")\n",
        "print(\"-\" * 60)\n",
        "for seg in result[\"segments\"]:\n",
        "    start = f\"{seg['start']:.1f}s\"\n",
        "    end = f\"{seg['end']:.1f}s\"\n",
        "    print(f\"{start:>8}  {end:>8}  {seg['text'].strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Transcription to File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_name = os.path.splitext(audio_file)[0]\n",
        "output_file = f\"{base_name}_transcription.txt\"\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Audio File: {audio_file}\\n\")\n",
        "    f.write(f\"Language: {detected_lang}\\n\")\n",
        "    f.write(f\"Model: {model_size}\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "    f.write(\"FULL TRANSCRIPTION:\\n\")\n",
        "    f.write(result[\"text\"] + \"\\n\\n\")\n",
        "    f.write(\"TIMESTAMPED SEGMENTS:\\n\")\n",
        "    for seg in result[\"segments\"]:\n",
        "        f.write(f\"[{seg['start']:.1f}s - {seg['end']:.1f}s] {seg['text'].strip()}\\n\")\n",
        "\n",
        "print(f\"Transcription saved to: {output_file}\")\n",
        "files.download(output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. (Optional) Translate Spanish Audio to English\n",
        "\n",
        "Whisper can also **translate** non-English audio directly to English text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Translate to English { run: \"auto\" }\n",
        "translate = False #@param {type:\"boolean\"}\n",
        "\n",
        "if translate:\n",
        "    print(f\"Translating '{audio_file}' to English...\")\n",
        "    translation = model.transcribe(audio_file, task=\"translate\")\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ENGLISH TRANSLATION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(translation[\"text\"])\n",
        "else:\n",
        "    print(\"Set 'translate' to True above and re-run this cell to translate.\")"
      ]
    }
  ]
}
